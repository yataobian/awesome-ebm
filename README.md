# Awesome Energy Based Models/Learning (Awesome-EBM)
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome#readme)
> A comprehensive list of energy based learning papers and materials.

## Table of Contents
- [Workshops & Symposiums](#workshops--symposiums)
- [Representative Applications](#representative-applications)
- [Papers (Reverse Chronological Order)](#papers-reverse-chronological-order))
    - [2022](#2022)
    - [2021](#2021)
    - [2020](#2020)
    - [2019](#2019)
    - [2017 ~ 2018](#2017--2018)
    - [2013 ~ 2016](#2013--2016)
    - [2007 ~ 2012](#2007--2012)
    - [Early papers (Before 2007)](#early-papers-before-2007)   
- [Tutorials & Talks & Blogs](#tutorials--talks--blogs)
- [Open Source Libraries](#open-source-libraries)


## Workshops & Symposiums

- [ ] [EBM Workshop at ICLR 2021.](https://sites.google.com/view/ebm-workshop-iclr2021/home)

## Representative Applications

- [ ] Data (image/graph/sequence/etc) generation
- [ ] Discriminative learning: Classification/regression
- [ ] Density estimation
- [ ] Maximum entropy reinforcement learning
- [ ] Out-of-distribution detection (OOD)/anomaly detection/fraud detection
- [ ] Model calibration
- [ ] Adversarial robustness
- [ ] Image inpainting/denoising/super-resolution
- [ ] Prior modeling
- [ ] Model-based planning for robotics
- [ ] Language/Speech modeling

## Papers (Reverse Chronological Order)

### 2023

- [ ] [Tobias Schröder, Zijing Ou, Jen Ning Lim, Yingzhen Li, Sebastian J. Vollmer, Andrew B. Duncan. (2023). \
Energy Discrepancies: A Score-Independent Loss for Energy-Based Models. In NeurIPS 2023.](https://arxiv.org/abs/2307.06431)

- [ ] [Mingtian Zhang, Alex Hawkins-Hooker, Brooks Paige, David Barber. (2023). \
Moment Matching Denoising Gibbs Sampling. In NeurIPS 2023.](https://arxiv.org/abs/2305.11650)

- [ ] [ Timur Garipov, Sebastiaan De Peuter, Ge Yang, Vikas Garg, Samuel Kaski, Tommi S. Jaakkola. (2023). \
Compositional Sculpting of Iterative Generative Processes. In NeurIPS 2023.](https://openreview.net/forum?id=w79RtqIyoM)

- [ ] [ P Yu, Y Zhu, S Xie, X Ma, R Gao, SC Zhu, YN Wu. (2023). \
Learning Energy-Based Prior Model with Diffusion-Amortized MCMC. In NeurIPS 2023.](https://arxiv.org/pdf/2310.03218.pdf)

- [ ] [Tobias Schröder, Zijing Ou, Yingzhen Li, Andrew B. Duncan. (2023). \
Training Discrete Energy-Based Models with Energy Discrepancy. In ICML SODS Workshop.](https://arxiv.org/abs/2307.07595)

- [ ] [ W Jiang, J Qin, L Wu, C Chen, T Yang, L Zhang. (2023). \
Learning Unnormalized Statistical Models via Compositional Optimization. In ICML 2023.](https://proceedings.mlr.press/v202/jiang23g/jiang23g.pdf)

- [ ] [Aaron Lou, Chenlin Meng, Stefano Ermon. (2023). \
Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution. arXiv preprint arXiv:2310.16834.](https://arxiv.org/abs/2310.16834)

### 2022

- [ ] [ Chenlin Meng, Kristy Choi, Jiaming Song, Stefano Ermon. (2022). \
Concrete Score Matching: Generalized Score Matching for Discrete Data. In NeurIPS 2022.](https://arxiv.org/abs/2211.00802)



- [ ] [ Min Zhao, Fan Bao, Chongxuan Li, Jun Zhu. (2022). \
EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations. In NeurIPS 2022.](https://arxiv.org/abs/2207.06635)



- [ ] [ Lianhui Qin, Sean Welleck, Daniel Khashabi, Yejin Choi. (2022). \
COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics. In NeurIPS 2022.](https://arxiv.org/abs/2202.11705)



- [ ] [ Lingkai Kong, Jiaming Cui, Yuchen Zhuang, Rui Feng, B. Aditya Prakash, Chao Zhang (2022). \
End-to-end Stochastic Programming with Energy-based Model. In NeurIPS 2022.](https://neurips.cc/virtual/2022/poster/52816)


- [ ] [ Xiao, Z., & Han, T. (2022). \
 Adaptive Multi-stage Density Ratio Estimation for Learning Latent Space Energy-based Model. In NeurIPS 2022.](https://arxiv.org/abs/2209.08739)



- [ ] [  Minsoo Kang, Hyewon Yoo, Eunhee Kang, Sehwan Ki, Hyong Euk Lee, Bohyung Han. (2022). \
 Information-Theoretic Generative Model Compression with Variational Energy-based Model. In NeurIPS 2022.](https://neurips.cc/virtual/2022/poster/54815)


- [ ] [  Blondel, M., Llinares-López, F., Dadashi, R., Hussenot, L., & Geist, M. (2022). \
 Learning Energy Networks with Generalized Fenchel-Young Losses. In NeurIPS 2022.](https://arxiv.org/pdf/2205.09589.pdf)


- [ ] [ Suri, K., Shi, X. Q., Plataniotis, K., & Lawryshyn, Y. (2020). \
 Energy-based surprise minimization for multi-agent value factorization. In NeurIPS 2022.](https://arxiv.org/abs/2009.09842)


- [ ] [ Kim, B., & Ye, J. C. (2022). \
 Energy-Based Contrastive Learning of Visual Representations. In NeurIPS 2022.](https://arxiv.org/abs/2202.04933)


- [ ] [Sun, H., Dai, H., Xia, W., & Ramamurthy, A. (2021, September). \
 Path Auxiliary Proposal for MCMC in Discrete Space. In International Conference on Learning Representations.](https://openreview.net/forum?id=JSR-YDImK95)

- [ ] [Eikema, B., Kruszewski, G., Elsahar, H., & Dymetman, M. (2021). \
 Sampling from Discrete Energy-Based Models with Quality/Efficiency Trade-offs. arXiv preprint arXiv:2112.05702.](https://arxiv.org/abs/2112.05702)

- [ ] [Liu, M., Liu, H., & Ji, S. (2021). \
Gradient-Guided Importance Sampling for Learning Discrete Energy-Based Models.](https://openreview.net/forum?id=IEKL-OihqX0)

- [ ] [Zhang, D., Malkin, N., Liu, Z., Volokhova, A., Courville, A., & Bengio, Y. (2022). \
Generative Flow Networks for Discrete Probabilistic Modeling. arXiv preprint arXiv:2202.01361.](https://arxiv.org/abs/2202.01361)

- [ ] [Zhang, R., Liu, X., & Liu, Q. (2022). \
A Langevin-like Sampler for Discrete Distributions. arXiv preprint arXiv:2206.09914.](https://arxiv.org/pdf/2206.09914.pdf)

- [ ] [ Xie, B., Yuan, L., Li, S., Liu, C. H., Cheng, X., & Wang, G. (2021). \
   Active Learning for Domain Adaptation: An Energy-based Approach. AAAI 2022](https://arxiv.org/abs/2112.01406)

- [ ] [Sansone, E. (2021). \
LSB: Local Self-Balancing MCMC in Discrete Spaces. arXiv preprint arXiv:2109.03867.](https://arxiv.org/abs/2109.03867)


- [ ] [Ou, Z., Xu, T., Su, Q., Li, Y., Zhao, P., & Bian, Y.  2022). \
Learning Neural Set Functions Under the Optimal Subset Oracle. In NeurIPS 2022 (Oral).](https://subsetselection.github.io/EquiVSet/)



### 2021

- [ ] [Lazaro-Gredilla, M., Dedieu, A., & George, D. (2021). \
Perturb-and-max-product: Sampling and learning in discrete energy-based models. Advances in Neural Information Processing Systems, 34.](https://arxiv.org/abs/2111.02458)

- [ ] [Yu, L., Song, J., Song, Y., & Ermon, S. (2021). \
  Pseudo-Spherical Contrastive Divergence. arXiv preprint arXiv:2111.00780.](https://arxiv.org/abs/2111.00780)

- [ ] [Sodhi, P., Dexheimer, E., Mukadam, M., Anderson, S., & Kaess, M. (2021). \
 LEO: Learning Energy-based Models in Graph Optimization. arXiv preprint arXiv:2108.02274.](https://arxiv.org/abs/2108.02274)

- [ ] [Xie, J., Zheng, Z., Fang, X., Zhu, S. C., & Wu, Y. N. (2021). \
 Cooperative training of fast thinking initializer and slow thinking solver for conditional learning. IEEE Transactions on Pattern Analysis and Machine Intelligence.](https://ieeexplore.ieee.org/abstract/document/9387560/)


- [ ] [Chen, P. H., Wei, W., Hsieh, C. J., & Dai, B. (2021, July). \
 Overcoming Catastrophic Forgetting by Bayesian Generative Regularization. In International Conference on Machine Learning (pp. 1760-1770). PMLR.](http://proceedings.mlr.press/v139/chen21v.html)


- [ ] [Jaini, P., Holdijk, L., & Welling, M. (2021). \
 Learning Equivariant Energy Based Models with Equivariant Stein Variational Gradient Descent. arXiv preprint arXiv:2106.07832.](https://arxiv.org/pdf/2106.07832.pdf)

- [ ] [Grathwohl, W., Swersky, K., Hashemi, M., Duvenaud, D., & Maddison, C. (2021, July). \
Oops i took a gradient: Scalable sampling for discrete distributions. In International Conference on Machine Learning (pp. 3831-3841). PMLR.](https://arxiv.org/abs/2102.04509)

- [ ] [Yoon, J., Hwang, S. J., & Lee, J. (2021). \
 Adversarial purification with Score-based generative models. arXiv preprint arXiv:2106.06041.](https://arxiv.org/abs/2106.06041)

- [ ] [Domingo-Enrich, C., Bietti, A., Vanden-Eijnden, E., & Bruna, J. (2021).\
 On Energy-Based Models with Overparametrized Shallow Neural Networks. arXiv preprint arXiv:2104.07531.](https://arxiv.org/abs/2104.07531)

- [ ] [2021: Durkan, Conor, and Yang Song.  \
On Maximum Likelihood Training of Score-Based Generative Models. ](https://arxiv.org/abs/2101.09258)

- [ ] [2021: Song, Y., & Kingma, D. P.  \
How to Train Your Energy-Based Models. arXiv preprint arXiv:2101.03288, 2021. ](https://arxiv.org/pdf/2101.03288.pdf)

- [ ] [Shi, C., Luo, S., Xu, M., & Tang, J. (2021). \
Learning Gradient Fields for Molecular Conformation Generation. arXiv preprint arXiv:2105.03902.](https://arxiv.org/abs/2105.03902)

- [ ] [Yang, X., & Ji, S. (2021). \
JEM++: Improved Techniques for Training JEM. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 6494-6503).](https://arxiv.org/abs/2109.09032)

- [ ] [Zhu, Y., Ma, J., Sun, J., Chen, Z., Jiang, R., Chen, Y., & Li, Z. (2021). \
 Towards understanding the generative capability of adversarially robust classifiers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 7728-7737).](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Towards_Understanding_the_Generative_Capability_of_Adversarially_Robust_Classifiers_ICCV_2021_paper.pdf)

- [ ] [Yang, X., Ye, H., Ye, Y., Li, X., & Ji, S. (2021). \
Generative Max-Mahalanobis Classifiers for Image Classification, Generation and More. arXiv preprint arXiv:2101.00122.](https://arxiv.org/abs/2101.00122)

- [ ] [Suhail, M., Mittal, A., Siddiquie, B., Broaddus, C., Eledath, J., Medioni, G., & Sigal, L. (2021). \
 Energy-Based Learning for Scene Graph Generation. arXiv preprint arXiv:2103.02221.](https://arxiv.org/abs/2103.02221)*[[code]](https://github.com/mods333/energy-based-scene-graph)*

- [ ] [Liu, M., Yan, K., Oztekin, B., & Ji, S. (2021).  \
GraphEBM: Molecular Graph Generation with Energy-Based Models. arXiv preprint arXiv:2102.00546.](https://arxiv.org/abs/2102.00546)

- [ ] [Wu, H., Esmaeili, B., Wick, M., Tristan, J. B., & van de Meent, J. W. (2021). \
Conjugate Energy-Based Models.](https://openreview.net/pdf?id=4k58RmAD02)

- [ ] [Zheng, Z., Xie, J., & Li, P. (2021). \
Patchwise Generative ConvNet: Training Energy-Based Models from a Single Natural Image for Internal Learning.](http://www.stat.ucla.edu/~jxie/personalpage_file/publications/internal_EBM.pdf)

- [ ] [Wu, Jiaxiang and Shen, Tao and Lan, Haidong and Bian, Yatao and Huang, Junzhou. (2021) \
SE(3)-Equivariant Energy-based Models for End-to-End Protein Folding.](https://www.biorxiv.org/content/10.1101/2021.06.06.447297)

- [ ] [Bian, Y., Rong, Y., Xu, T., Wu, J., Krause, A., & Huang, J. (2021). \
 Energy-Based Learning for Cooperative Games, with Applications to Feature/Data/Model Valuations. arXiv preprint arXiv:2106.02938.](https://arxiv.org/abs/2106.02938)


### 2020

- [ ] [Grathwohl, W., Wang, K. C., Jacobsen, J. H., Duvenaud, D., & Zemel, R. (2020, November). \
 Learning the stein discrepancy for training and evaluating energy-based models without sampling. In International Conference on Machine Learning (pp. 3732-3747). PMLR.](http://proceedings.mlr.press/v119/grathwohl20a.html)

- [ ] [Du, Y., Li, S., & Mordatch, I. (2020). \
 Compositional visual generation with energy based models. Advances in Neural Information Processing Systems, 33, 6637-6647.](https://proceedings.neurips.cc/paper_files/paper/2020/hash/49856ed476ad01fcff881d57e161d73f-Abstract.html)

- [ ] [Du, Y., Li, S., Tenenbaum, J., & Mordatch, I. (2020). \
Improved contrastive divergence training of energy based models. arXiv preprint arXiv:2012.01316.](https://arxiv.org/abs/2012.01316)

- [ ] [Meng, C., Yu, L., Song, Y., Song, J., & Ermon, S. (2020). \
 Autoregressive Score Matching. arXiv preprint arXiv:2010.12810.](https://arxiv.org/abs/2010.12810)

- [ ] [Clark, K., Luong, M. T., Le, Q. V., & Manning, C. D. (2020). \
 Pre-Training Transformers as Energy-Based Cloze Models. arXiv preprint arXiv:2012.08561.](https://arxiv.org/abs/2012.08561)

- [ ] [Boney, R., Kannala, J., & Ilin, A. (2020, May). \
 Regularizing model-based planning with energy-based models. In Conference on Robot Learning (pp. 182-191). PMLR.](http://proceedings.mlr.press/v100/boney20a.html)


- [ ] [Liu, M., He, T., Xu, M., & Zhang, W. (2020). \
  Energy-based imitation learning. arXiv preprint arXiv:2004.09395.](https://arxiv.org/abs/2004.09395)


- [ ] [Xiao, Z., Kreis, K., Kautz, J., & Vahdat, A. (2020, September). \
 Vaebm: A symbiosis between variational autoencoders and energy-based models. In International Conference on Learning Representations.](https://openreview.net/forum?id=5m3SEczOV8L)


- [ ] [ Bao, F., Xu, K., Li, C., Hong, L., Zhu, J., & Zhang, B. (2020). \
Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models. arXiv preprint arXiv:2010.08258.](https://arxiv.org/abs/2010.08258)


- [ ] [Dai, H., Singh, R., Dai, B., Sutton, C., & Schuurmans, D. (2020). \
 Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration. arXiv preprint arXiv:2011.05363.](https://arxiv.org/abs/2011.05363)

- [ ] [2020: Gustafsson, F. K., Danelljan, M., Timofte, R., and Schön, T. B. \
How to Train Your Energy-Based Model for Regression. arXiv preprint arXiv:2005.01698, 2020.](https://arxiv.org/pdf/2005.01698.pdf)

- [ ] [Gustafsson, F. K., Danelljan, M., Bhat, G., & Schön, T. B. (2020, August).\
 Energy-based models for deep probabilistic regression. In European Conference on Computer Vision (pp. 325-343). Springer, Cham.](https://link.springer.com/chapter/10.1007/978-3-030-58565-5_20)

- [ ] [2020: Niu, C., Song, Y., Song, J., Zhao, S., Grover, A., and Ermon, S. \
Permutation invariant graph generation via score-Based generative modeling. In International Conference on Artificial Intelligence and Statistics (pp. 4474-4484). PMLR, 2020.](https://arxiv.org/pdf/2003.00638.pdf) *[[code]](https://github.com/ermongroup/GraphScoreMatching)*

- [ ] [2020: Liu, H., and Abbeel, P. \
Hybrid discriminative-generative training via contrastive learning. arXiv preprint arXiv:2007.09070, 2020.](https://arxiv.org/pdf/2007.09070.pdf)

- [ ] [Ho, J., Jain, A., & Abbeel, P. (2020). \
Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239.](https://arxiv.org/abs/2006.11239)

- [ ] [2020: Arbel, Michael, Zhou, Liang, and  Gretton,  Arthur.  \
 Generalized energy based models.arXiv e-prints,pp. arXiv–2003, 2020.](https://arxiv.org/pdf/2003.05033.pdf)

- [ ] [2020: Song,  Yang,  Sohl-Dickstein,  Jascha,  Kingma,  Diederik P, Kumar,  Abhishek,  Ermon,  Ste-fano, and Poole, Ben.   \
Score-based generative modeling through stochastic differential equations.arXiv preprint arXiv:2011.13456, 2020.](https://arxiv.org/pdf/2011.13456.pdf)

- [ ] [2020: Song, Yang, and Stefano Ermon.  \
Improved techniques for training score-based generative models." arXiv preprint arXiv:2006.09011 (2020).](https://arxiv.org/abs/2006.09011)

- [ ] [2020: Gao, Ruiqi, et al. \
Flow contrastive estimation of energy-based models. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.](https://openaccess.thecvf.com/content_CVPR_2020/papers/Gao_Flow_Contrastive_Estimation_of_Energy-Based_Models_CVPR_2020_paper.pdf)

- [ ] [2020: Khemakhem, Ilyes, et al. \
ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA. Advances in Neural Information Processing Systems 33 (2020).](https://arxiv.org/abs/2002.11537) *[[code]](https://github.com/ilkhem/icebeem)*

- [ ] [2020: Cotta, L., Teixeira, C. H., Swami, A., & Ribeiro, B. (2020). \
 Unsupervised Joint $k$-node Graph Representations with Compositional Energy-Based Models. arXiv preprint arXiv:2010.04259.](https://arxiv.org/abs/2010.04259)


- [ ] [2020:  Pang, B., Han, T., Nijkamp, E., Zhu, S. C., & Wu, Y. N. (2020).\
Learning latent space energy-based prior model. arXiv preprint arXiv:2006.08205.](https://arxiv.org/abs/2006.08205)


- [ ] [Liu, W., Wang, X., Owens, J. D., & Li, Y. (2020).\
 Energy-based Out-of-distribution Detection. arXiv preprint arXiv:2010.03759.](https://arxiv.org/abs/2010.03759)

- [ ] [Nijkamp, E., Hill, M., Han, T., Zhu, S. C., & Wu, Y. N. (2020, April). \
    On the anatomy of mcmc-based maximum likelihood learning of energy-based models. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 04, pp. 5272-5280).](https://ojs.aaai.org/index.php/AAAI/article/view/5973)

- [ ] [Cai, R., Yang, G., Averbuch-Elor, H., Hao, Z., Belongie, S., Snavely, N., & Hariharan, B. (2020).\
Learning gradient fields for shape generation. arXiv preprint arXiv:2008.06520.](https://arxiv.org/abs/2008.06520)

- [ ] [Wang, Z., Cheng, S., Yueru, L., Zhu, J., & Zhang, B. (2020, June).\
 A wasserstein minimum velocity approach to learning unnormalized models. In International Conference on Artificial Intelligence and Statistics (pp. 3728-3738). PMLR.](http://proceedings.mlr.press/v108/wang20j)

- [ ] [Che, T., Zhang, R., Sohl-Dickstein, J., Larochelle, H., Paull, L., Cao, Y., & Bengio, Y. (2020).\
 Your gan is secretly an energy-based model and you should use discriminator driven latent sampling. arXiv preprint arXiv:2003.06060.](https://arxiv.org/abs/2003.06060)


- [ ] [Grathwohl, W., Kelly, J., Hashemi, M., Norouzi, M., Swersky, K., & Duvenaud, D. (2020).\
 No MCMC for me: Amortized sampling for fast and stable training of energy-based models. arXiv preprint arXiv:2010.04230.](https://arxiv.org/abs/2010.04230)

- [ ] [Yu, L., Song, Y., Song, J., & Ermon, S. (2020, November).\
  Training deep energy-based models with f-divergence minimization. In International Conference on Machine Learning (pp. 10957-10967). PMLR.](http://proceedings.mlr.press/v119/yu20g.html)

- [ ] [Li, S., Du, Y., van de Ven, G. M., Torralba, A., & Mordatch, I. (2020).\
 Energy-Based Models for Continual Learning. arXiv preprint arXiv:2011.12216.](https://arxiv.org/abs/2011.12216)


- [ ] [Du, Y., Meier, J., Ma, J., Fergus, R., & Rives, A. (2020).\
   Energy-based models for atomic-resolution protein conformations. arXiv preprint arXiv:2004.13167.](https://arxiv.org/abs/2004.13167)

- [ ] [Deng, Y., Bakhtin, A., Ott, M., Szlam, A., & Ranzato, M. A. (2020).\
 Residual energy-based models for text generation. arXiv preprint arXiv:2004.11714.](https://arxiv.org/abs/2004.11714)

- [ ] [Sahin, A., Bian, Y., Buhmann, J., & Krause, A. (2020, November).\
From Sets to Multisets: Provable Variational Inference for Probabilistic Integer Submodular Models. In International Conference on Machine Learning (pp. 8388-8397). PMLR.](http://proceedings.mlr.press/v119/sahin20a.html)


### 2019

- [ ] [Saremi, S., & Hyvärinen, A. (2019).\
 Neural empirical bayes. Journal of Machine Learning Research, 20, 1-23.](https://www.jmlr.org/papers/volume20/19-216/19-216.pdf)

- [ ] [2019: Song, Y., and Ermon, S.  \
Generative Modeling by Estimating Gradients of the Data Distribution. In Proceedings of the 33rd Annual Conference on Neural Information Processing Systems, 2019.](https://arxiv.org/pdf/1907.05600.pdf) *[[code]](https://github.com/ermongroup/ncsn)*

- [ ] [Wenliang, L., Sutherland, D., Strathmann, H., & Gretton, A. (2019, May).\
Learning deep kernels for exponential family densities. In International Conference on Machine Learning (pp. 6737-6746). PMLR.](http://proceedings.mlr.press/v97/wenliang19a.html)

- [ ] [2019: Grathwohl, W., Wang, K. C., Jacobsen, J. H., Duvenaud, D., Norouzi, M., & Swersky, K.  \
Your classifier is secretly an energy based model and you should treat it like one. arXiv preprint arXiv:1912.03263, 2019.](https://arxiv.org/pdf/1912.03263.pdf)  *[[code]](https://github.com/wgrathwohl/JEM)*

- [ ] [2019: Bian, Y., Buhmann, J., & Krause, A. \
 Optimal continuous dr-submodular maximization and applications to provable mean field inference. In International Conference on Machine Learning (pp. 644-653). PMLR.](http://proceedings.mlr.press/v97/bian19a.html)  *[[code]](https://github.com/bianan/optimal-dr-submodular-max)*

- [ ] [2019: Du, Yilun, and Igor Mordatch. \
 Implicit generation and modeling with energy based models. (2019).](https://arxiv.org/pdf/1903.08689.pdf)

- [ ] [Nijkamp, E., Hill, M., Zhu, S. C., & Wu, Y. N. (2019). \
  Learning non-convergent non-persistent short-run MCMC toward energy-based model. arXiv preprint arXiv:1904.09770.](https://arxiv.org/abs/1904.09770)

- [ ] [Dai, B., Liu, Z., Dai, H., He, N., Gretton, A., Song, L., & Schuurmans, D. (2019). \
 Exponential family estimation via adversarial dynamics embedding. arXiv preprint arXiv:1904.12083.](https://arxiv.org/abs/1904.12083)

- [ ] [Bartunov, S., Rae, J. W., Osindero, S., & Lillicrap, T. P. (2019). \
   Meta-learning deep energy-based memory models. arXiv preprint arXiv:1910.02720.](https://arxiv.org/abs/1910.02720)



### 2017 ~ 2018

- [ ] [Mordatch, I. (2018). \
 Concept learning with energy-based models. arXiv preprint arXiv:1811.02486.](https://arxiv.org/abs/1811.02486)  *[[blog]](https://openai.com/blog/learning-concepts-with-energy-functions/)*


- [ ] [Xie, J., Lu, Y., Gao, R., Zhu, S. C., & Wu, Y. N. (2018). \
 Cooperative training of descriptor and generator networks. IEEE transactions on pattern analysis and machine intelligence, 42(1), 27-45.](https://arxiv.org/abs/1609.09408) *[[code]](https://github.com/jianwen-xie/CoopNets)*

- [ ] [Gao, R., Lu, Y., Zhou, J., Zhu, S. C., & Wu, Y. N. (2018). \
 Learning generative convnets via multi-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 9155-9164).](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gao_Learning_Generative_ConvNets_CVPR_2018_paper.pdf) *[[code]](https://github.com/prateekm08/Multigrid-PyTorch)*

- [ ] [Yoon, K., Liao, R., Xiong, Y., Zhang, L., Fetaya, E., Urtasun, R., ... & Pitkow, X. (2018). \
 Inference in Probabilistic Graphical Models by Graph Neural Networks. arXiv preprint arXiv:1803.07710.](https://arxiv.org/abs/1803.07710)


- [ ] [Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018, July). \
Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning (pp. 1861-1870). PMLR.](http://proceedings.mlr.press/v80/haarnoja18b)

- [ ] [ Ceylan, C., & Gutmann, M. U. (2018, July).\
 Conditional noise-contrastive estimation of unnormalised models. In International Conference on Machine Learning (pp. 726-734). PMLR.](http://proceedings.mlr.press/v80/ceylan18a.html)

- [ ] [Bose, A. J., Ling, H., & Cao, Y. (2018).\
Adversarial contrastive estimation. arXiv preprint arXiv:1805.03642.](https://arxiv.org/abs/1805.03642)


- [ ] [Scellier, B., & Bengio, Y. (2017).\
 Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. Frontiers in computational neuroscience, 11, 24.](https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full)

- [ ] [Haarnoja, T., Tang, H., Abbeel, P., & Levine, S. (2017, July).\
 Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning (pp. 1352-1361). PMLR.](http://proceedings.mlr.press/v70/haarnoja17a.html)



### 2013 ~ 2016


- [ ] [2016: Zhao, J., Mathieu, M., & LeCun, Y. (2016). \
  Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126.](https://arxiv.org/abs/1609.03126)


- [ ] [Xie, J., Lu, Y., Zhu, S. C., & Wu, Y. (2016, June). \
A theory of generative convnet. In International Conference on Machine Learning (pp. 2635-2644). PMLR.](http://proceedings.mlr.press/v48/xiec16.html)

- [ ] [Liu, Q., Lee, J., & Jordan, M. (2016, June).  \
A kernelized Stein discrepancy for goodness-of-fit tests. In International conference on machine learning (pp. 276-284). PMLR.](http://proceedings.mlr.press/v48/liub16.html)


- [ ] [Zhai, S., Cheng, Y., Feris, R., & Zhang, Z. (2016).  \
 Generative adversarial networks as variational training of energy based models. arXiv preprint arXiv:1611.01799.](https://arxiv.org/abs/1611.01799)


- [ ] [Kim, T., & Bengio, Y. (2016). \
   Deep directed generative models with energy-based probability estimation. arXiv preprint arXiv:1606.03439.](https://arxiv.org/abs/1606.03439)


- [ ] [Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015, June). \
Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (pp. 2256-2265). PMLR.](http://proceedings.mlr.press/v37/sohl-dickstein15.html)

- [ ] [Gorham, J., & Mackey, L. (2015).\
 Measuring sample quality with Stein's method. arXiv preprint arXiv:1506.03039.](https://arxiv.org/abs/1506.03039)





### 2007 ~ 2012

- [ ] [2012: Parry, Matthew, Dawid, A Philip, Lauritzen, Steffen, et al.  \
 Proper local scoring rules.Annals of Statistics, 40(1):561–592, 2012.](http://www.stats.ox.ac.uk/~steffen/papers/AOS971.pdf)

- [ ] [Gutmann, M., & Hirayama, J. I. (2012).\
  Bregman divergence as general framework to estimate unnormalized statistical models. arXiv preprint arXiv:1202.3727.](https://arxiv.org/abs/1202.3727)

- [ ] [Welling, M., & Teh, Y. W. (2011).\
  Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 681-688).](http://www.stats.ox.ac.uk/~steffen/papers/AOS971.pdf)

- [ ] [Raphan, M., & Simoncelli, E. P. (2011).\
 Least squares estimation without priors or supervision. Neural computation, 23(2), 374-420.](https://direct.mit.edu/neco/article/23/2/374/7627/Least-Squares-Estimation-Without-Priors-or)

- [ ] [Sohl-Dickstein, J., Battaglino, P., & DeWeese, M. R. (2011, January).\
Minimum Probability Flow Learning. In ICML.](https://openreview.net/forum?id=Bk4fEjZd-S)


- [ ] [Vincent, P. (2011).\
 A connection between score matching and denoising autoencoders. Neural computation, 23(7), 1661-1674.](http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf)


- [ ] [Lyu, S. (2011, September).\
Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction. In NIPS (pp. 64-72).](https://www.researchgate.net/profile/Siwei-Lyu/publication/266487843_Unifying_Non-Maximum_Likelihood_Learning_Objectives_with_Minimum_KL_Contraction/links/5661c8b608ae192bbf8ba332/Unifying-Non-Maximum-Likelihood-Learning-Objectives-with-Minimum-KL-Contraction.pdf)

- [ ] [Ngiam, J., Chen, Z., Koh, P. W., & Ng, A. Y. (2011, June).\
 Learning deep energy models. In Proceedings of the 28th International Conference on International Conference on Machine Learning (pp. 1105-1112).](https://openreview.net/forum?id=H1VeJhWdWS)



- [ ] [Gutmann, M., & Hyvärinen, A. (2010, March).\
 Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (pp. 297-304). JMLR Workshop and Conference Proceedings.](http://proceedings.mlr.press/v9/gutmann10a)


- [ ] [Kingma, D. P., & LeCun, Y. (2010, January).\
 Regularized estimation of image statistics by Score Matching. In NIPS (Vol. 509, p. 618).](http://dpkingma.com/files/kingma-lecun-nips-10.pdf)


- [ ] [Lyu, S. (2009, June). \
Interpretation and generalization of score matching. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (pp. 359-366).](https://dl.acm.org/doi/abs/10.5555/1795114.1795156)


- [ ] [Javier R Movellan Jan 2008.\
  A minimum velocity approach to learning. unpublished draft]()


- [ ] [Raphan, M., Simoncelli, E. P., Scholkopf, B., Platt, J., & Hoffman, T. (2007).\
 Learning to be Bayesian without supervision. Advances in neural information processing systems, 19, 1145.](https://proceedings.neurips.cc/paper/2006/file/908c9a564a86426585b29f5335b619bc-Paper.pdf)



- [ ] [Hyvarinen, A. (2007).\
 Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables. IEEE Transactions on neural networks, 18(5), 1529-1531.](https://ieeexplore.ieee.org/abstract/document/4298117)

- [ ] [Hyvarinen, A. (2007).\
Some extensions of score matching. Computational statistics & data analysis, 51(5), 2499-2512.](https://www.cs.helsinki.fi/u/ahyvarin/papers/CSDA07.pdf)



### Early papers (Before 2007)


- [ ] [Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). \
 A fast learning algorithm for deep belief nets. Neural computation, 18(7), 1527-1554.](https://direct.mit.edu/neco/article/18/7/1527/7065/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets)

- [ ] [LeCun, Y., & Huang, F. J. (2005, January).   \
Loss Functions for Discriminative Training of Energy-Based Models. In AIStats (Vol. 6, p. 34).](http://yann.lecun.com/exdb/publis/pdf/lecun-huang-05.pdf)


- [ ] [Hyvärinen, A., & Dayan, P. (2005).\
Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4).](http://yann.lecun.com/exdb/publis/pdf/lecun-huang-05.pdf)



- [ ] [Welling, M., Rosen-Zvi, M., & Hinton, G. E. (2004, December). \
Exponential Family Harmoniums with an Application to Information Retrieval. In Nips (Vol. 4, pp. 1481-1488).](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.86.5757&rep=rep1&type=pdf)

- [ ] [Teh, Y. W., Welling, M., Osindero, S., & Hinton, G. E. (2003).  \
Energy-based models for sparse overcomplete representations. Journal of Machine Learning Research, 4(Dec), 1235-1260.](https://www.jmlr.org/papers/volume4/teh03a/teh03a.pdf)


- [ ] [Hinton, G. E. (2002). \
Training products of experts by minimizing contrastive divergence. Neural computation, 14(8), 1771-1800.](https://direct.mit.edu/neco/article/14/8/1771/6687/Training-Products-of-Experts-by-Minimizing)


- [ ] [Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). \
 The helmholtz machine. Neural computation, 7(5), 889-904.](https://direct.mit.edu/neco/article-abstract/7/5/889/5898)


- [ ] [Smolensky, P. (1986).  \
Information processing in dynamical systems: Foundations of harmony theory. Colorado Univ at Boulder Dept of Computer Science.](https://apps.dtic.mil/sti/citations/ADA620727)


- [ ] [Stein, C. M. (1981). \
 Estimation of the mean of a multivariate normal distribution. The annals of Statistics, 1135-1151.](https://www.jstor.org/stable/2240405?seq=1)


- [ ] [1957a: Jaynes, Edwin T. \
Information theory and statistical mechanics.Physical review, 106(4):620,1957a](https://journals.aps.org/pr/abstract/10.1103/PhysRev.106.620)

- [ ] [1957b: Jaynes, Edwin T.   \
Information theory and statistical mechanics. ii. Physical review, 108(2):171, 1957b.](https://journals.aps.org/pr/abstract/10.1103/PhysRev.108.171)


## Tutorials & Talks & Blogs

- [ ] [2006: LeCun,  Yann,  Chopra,  Sumit,  Hadsell,  Raia,  Ranzato,  M,  and  Huang,  F.   \
A  tutorial  on energy-based learning. Predicting structured data, 1(0), 2006](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)

- [ ] [Yann LeCun, Sumit Chopra, Raia Hadsell, Fu-Jie Huang, Marc'Aurelio Ranzato (Courant Institute/CBLL), 2003. \
LeCun's research page on EBMs.](https://cs.nyu.edu/~yann/research/ebm/)

- [ ] [Takayuki Osogami, Sakyasingha Dasgupta, 2017. \
IJCAI-17 Tutorial: Energy-based machine learning.](https://researcher.watson.ibm.com/researcher/view_group.php?id=7834)

- [ ] [2020 Youtube video: Arthur Gretton. \
On the critic function of implicit generative models.](https://www.youtube.com/watch?v=et6Kgh6mWmc)

- [ ] [2020 Youtube video: Stefano Ermon. \
Generative Modeling by Estimating Gradients of the Data Distribution](https://www.youtube.com/watch?v=8TcNXi3A5DI&t=3409s)

- [ ] [UvA Deep Learning Tutorials Fall 2020.   \
Tutorial 8: Deep Energy-Based Generative Models](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html)


## Open Source Libraries

- [ ] [2020 Mateus Roder and Gustavo Henrique de Rosa and João Paulo Papa. \
Learnergy: Energy-based Machine Learners.](https://github.com/gugarosa/learnergy)

- [ ] [Maes, F. (2009). \
 Nieme: Large-scale energy-based models. The Journal of Machine Learning Research, 10, 743-746.](https://www.jmlr.org/papers/volume10/maes09a/maes09a.pdf)*[[code (however is missing in the webpage)]](http://nieme.lip6.fr/)*
